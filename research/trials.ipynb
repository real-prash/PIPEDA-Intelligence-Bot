{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291d8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a17350be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prash\\anaconda3\\envs\\legalbot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\prash\\anaconda3\\envs\\legalbot\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:47: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  from google.generativeai.caching import CachedContent  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    transport=\"rest\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"LLM Initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dde879d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 66 pages from PIPEDA Act.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "\n",
    "def load_pipeda(data_path):\n",
    "    loader = DirectoryLoader(data_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "docs = load_pipeda(\"data/\")\n",
    "print(f\"Loaded {len(docs)} pages from PIPEDA Act.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7da7404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Created 389 chunks.\n",
      "Sample Content: Current to December 29, 2025\n",
      "Last amended on March 4, 2025\n",
      "À jour au 29 décembre 2025\n",
      "Dernière modif...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 800 chars \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "text_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\" Created {len(text_chunks)} chunks.\")\n",
    "# first chunk\n",
    "print(f\"Sample Content: {text_chunks[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704218c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pinecone Index 'pipeda-bot-huggingface' is Ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Load Local Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Setup Pinecone\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = \"pipeda-bot-huggingface\"\n",
    "\n",
    "# Create index \n",
    "if index_name not in [idx.name for idx in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384, \n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "print(f\" Pinecone Index '{index_name}' is Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d20a97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully indexed PIPEDA into Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# Create vectorstore and upload\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "print(\" Successfully indexed PIPEDA into Pinecone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253ab1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI LEGAL ANALYSIS:\n",
      "\n",
      "Under PIPEDA, the rules for individual access to personal information are as follows:\n",
      "\n",
      "Upon request, an individual must be informed of the existence, use, and disclosure of their personal information and shall be given access to that information. Additionally, an individual has the right to challenge the accuracy and completeness of their information and have it amended as appropriate.\n",
      "\n",
      "However, there are exceptions to this access requirement. In certain situations, an organization may not be able to provide access to all the personal information it holds about an individual. These exceptions should be limited and specific, and the reasons for denying access must be provided to the individual upon request. One example of an exception mentioned is information that is prohibitively costly to provide.\n",
      "\n",
      "**Citation:** 4.9 Principle 9 — Individual Access\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a professional Canadian Legal Assistant specializing in PIPEDA. \"\n",
    "    \"Use the following retrieved context from the PIPEDA Act to answer the question. \"\n",
    "    \"If you cannot answer based on the context, say you do not know. \"\n",
    "    \"Always cite the specific Section or Schedule.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"input\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"What are the rules for individual access to personal information?\"\n",
    "\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(\" AI LEGAL ANALYSIS:\\n\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae41651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legalbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
